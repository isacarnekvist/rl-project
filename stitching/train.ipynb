{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "import gym\n",
    "from pendulum.rl import load_from_path\n",
    "from stitching.rl import PolicyCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = PolicyCollection()\n",
    "\n",
    "policy_path = '../pendulum/trained_agents/pendulum_00/policy.pkl'\n",
    "policy, env = load_from_path(policy_path)\n",
    "base_path = '../pendulum/trained_agents/'\n",
    "for folder in sorted(os.listdir(base_path)):\n",
    "    path = os.path.join(base_path, folder, 'policy.pkl')\n",
    "    collection.append(*load_from_path(path))\n",
    "\n",
    "collection.reset()\n",
    "dataset = []\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "for _ in range(10):\n",
    "    for x in collection.step():\n",
    "        dataset.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "covariance_matrix must be at least two-dimensional, with optional leading batch dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-fd2cfaec3921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLatentWrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-fd2cfaec3921>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, collection, q, z_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'std_logits'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.0/envs/rl-project/lib/python3.5/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcovariance_matrix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 raise ValueError(\"covariance_matrix must be at least two-dimensional, \"\n\u001b[0m\u001b[1;32m    136\u001b[0m                                  \"with optional leading batch dimensions\")\n\u001b[1;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: covariance_matrix must be at least two-dimensional, with optional leading batch dimensions"
     ]
    }
   ],
   "source": [
    "class QFunction(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, collection, z_size):\n",
    "        super(QFunction, self).__init__()\n",
    "        x_size = env.observation_space.shape[0]\n",
    "        u_size = env.action_space.shape[0]\n",
    "        self.fc1 = torch.nn.Linear(x_size + z_size, 400)\n",
    "        self.fc2 = torch.nn.Linear(400 + u_size, 300)\n",
    "        self.fc3 = torch.nn.Linear(300, 1)\n",
    "    \n",
    "    def forward(self, x, u, z):\n",
    "        xz = torch.cat([x, z], dim=1)\n",
    "        a1 = F.relu(self.fc1(xz))\n",
    "        a1u = torch.cat([a1, u], dim=1)\n",
    "        a2 = F.relu(self.fc2(a1u))\n",
    "        y = self.fc3(a2)\n",
    "        return y\n",
    "    \n",
    "class LatentWrapped(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, collection, q, z_size):\n",
    "        super(LatentWrapped, self).__init__()\n",
    "        self.z_size = z_size\n",
    "        n = len(collection.envs)\n",
    "        self.register_buffer('mean', torch.randn(n, z_size))\n",
    "        self.register_buffer('std_logits', torch.randn(n, z_size))\n",
    "        self.q = q\n",
    "        \n",
    "    def forward(self, i, x, u):\n",
    "        eps = torch.randn(x.size(0), self.z_size)\n",
    "        z = self.mean[i] + eps * torch.exp(self.std_logits[i])\n",
    "        return self.q(x, u, z)\n",
    "        \n",
    "    \n",
    "z_size = 4\n",
    "q = QFunction(collection, z_size)\n",
    "inds, o, u, r, o_, u_ = next(iter(dataloader))\n",
    "z = torch.randn(o.size(0), z_size)\n",
    "wrapped = LatentWrapped(collection, q, 4)\n",
    "wrapped(inds, o, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    0     0     0     0     0     0     0     0     0     1\n",
      "    0     0     0     1     0     0     0     0     0     0\n",
      "    0     0     0     0     0     1     0     0     0     0\n",
      "    0     0     0     0     0     1     0     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     0     0     1     0\n",
      "    1     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     1\n",
      "    0     0     0     0     0     1     0     0     0     0\n",
      "    1     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     1     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     1     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     0     1     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     1     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     1     0\n",
      "    0     0     0     0     0     0     0     0     0     1\n",
      "    0     0     0     1     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0     0     0     0\n",
      "    0     1     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     1     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     1\n",
      "    0     0     0     0     0     0     0     0     1     0\n",
      "    0     0     0     0     1     0     0     0     0     0\n",
      "    0     0     1     0     0     0     0     0     0     0\n",
      "    0     0     1     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     1     0     0     0     0     0     0     0     0\n",
      "    0     0     0     1     0     0     0     0     0     0\n",
      "    0     0     1     0     0     0     0     0     0     0\n",
      "    0     1     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     1     0     0     0     0     0\n",
      "    0     0     0     0     1     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     1\n",
      "    0     0     0     0     0     0     0     1     0     0\n",
      "    1     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     1\n",
      "    0     1     0     0     0     0     0     0     0     0\n",
      "    0     0     0     1     0     0     0     0     0     0\n",
      "    0     0     0     0     1     0     0     0     0     0\n",
      "    0     0     1     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     1\n",
      "    1     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     1     0\n",
      "    0     0     0     0     0     0     0     1     0     0\n",
      "    0     0     0     0     0     0     0     0     1     0\n",
      "    0     0     0     0     0     1     0     0     0     0\n",
      "    0     0     0     1     0     0     0     0     0     0\n",
      "    0     0     0     0     1     0     0     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     0     0     1     0\n",
      "    0     0     0     0     0     1     0     0     0     0\n",
      "    0     0     0     0     1     0     0     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    1     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     1     0     0     0     0\n",
      "    0     1     0     0     0     0     0     0     0     0\n",
      "    0     0     0     1     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     1     0     0\n",
      "    0     0     0     0     1     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     1     0     0\n",
      "[torch.FloatTensor of size 64x10]\n",
      ", \n",
      " 0.7872  0.6015\n",
      " 1.7990  2.5153\n",
      " 1.8870  0.7816\n",
      " 1.8030  0.0756\n",
      "-2.2288 -1.1282\n",
      " 0.2780 -0.3405\n",
      "-3.1143 -1.4908\n",
      " 0.9741  1.4813\n",
      " 1.8396 -0.6212\n",
      "-2.7475 -1.0274\n",
      "-2.3944 -2.1850\n",
      "-1.1740  0.1202\n",
      " 0.2647 -0.2649\n",
      "-2.1724 -1.0263\n",
      " 2.2959  1.4852\n",
      "-2.1210 -0.9939\n",
      " 1.6876  1.1250\n",
      "-2.8554 -1.2062\n",
      " 0.1945 -0.4386\n",
      " 1.1556  1.9428\n",
      " 1.3751 -0.4391\n",
      "-2.6450 -0.7264\n",
      "-1.2634  0.6748\n",
      " 1.8706 -0.8014\n",
      " 0.9000  1.3001\n",
      " 0.1356 -0.3450\n",
      " 1.5707  0.0526\n",
      "-2.3087 -0.0742\n",
      "-2.3092  0.1127\n",
      "-2.2852 -1.1277\n",
      "-1.1783 -0.0848\n",
      " 1.9415  2.8517\n",
      "-2.2932 -0.0809\n",
      "-1.2080 -0.3428\n",
      " 1.8526  1.8250\n",
      " 2.2711  3.3362\n",
      " 0.8350  0.9562\n",
      " 2.1092  0.8223\n",
      "-2.7950 -0.9514\n",
      " 1.0584  1.6866\n",
      "-1.1931  0.3530\n",
      " 1.3876  0.3378\n",
      " 1.6314  0.7796\n",
      "-2.3130 -0.0857\n",
      " 1.6605  4.2611\n",
      "-2.9181 -1.2553\n",
      " 0.1705 -0.4807\n",
      " 2.0161  0.2513\n",
      " 0.2383 -0.2643\n",
      " 1.8040 -0.2663\n",
      " 1.6732  2.1634\n",
      " 1.9698  2.3438\n",
      "-2.8234 -3.2189\n",
      " 0.2165 -0.4370\n",
      " 1.9344  0.9466\n",
      " 1.5681 -0.3277\n",
      "-2.5201 -2.5139\n",
      "-2.6961 -1.0218\n",
      " 1.7992 -0.0958\n",
      "-1.1800  0.2610\n",
      " 1.5650  1.6500\n",
      " 2.0681  0.6062\n",
      " 1.5924  0.4330\n",
      " 2.0377  0.4323\n",
      "[torch.FloatTensor of size 64x2]\n",
      ", \n",
      "-0.8000\n",
      "-2.0000\n",
      "-1.9200\n",
      "-1.6400\n",
      " 1.8400\n",
      "-0.6400\n",
      "-1.6400\n",
      "-1.8800\n",
      "-1.3548\n",
      " 1.8800\n",
      " 0.5600\n",
      " 1.4800\n",
      " 0.6124\n",
      " 1.6000\n",
      "-1.5200\n",
      " 1.8800\n",
      "-1.8800\n",
      " 0.8400\n",
      "-0.9200\n",
      "-1.3641\n",
      "-1.9600\n",
      " 0.3200\n",
      "-1.1645\n",
      "-1.8800\n",
      "-1.8400\n",
      "-0.1200\n",
      "-1.7600\n",
      " 1.5200\n",
      " 0.2808\n",
      "-1.5200\n",
      " 1.6000\n",
      "-1.4473\n",
      " 1.5600\n",
      " 1.8800\n",
      "-0.9600\n",
      "-0.8400\n",
      "-0.9600\n",
      "-1.9600\n",
      " 0.0000\n",
      "-1.8000\n",
      " 1.8400\n",
      "-0.9868\n",
      "-1.9200\n",
      " 1.5600\n",
      " 1.6800\n",
      " 1.5200\n",
      " 0.0000\n",
      "-1.8800\n",
      "-1.7200\n",
      "-1.9600\n",
      "-1.5585\n",
      "-1.6400\n",
      " 0.4400\n",
      "-0.8000\n",
      "-1.8400\n",
      "-0.2456\n",
      " 0.3200\n",
      "-1.5918\n",
      "-1.9600\n",
      " 1.6800\n",
      "-1.2000\n",
      "-1.6800\n",
      "-1.9200\n",
      "-1.8800\n",
      "[torch.FloatTensor of size 64x1]\n",
      ", \n",
      " -0.7892\n",
      " -4.5867\n",
      " -3.8336\n",
      " -3.3248\n",
      " -5.3514\n",
      " -0.0773\n",
      " -9.8066\n",
      " -1.4077\n",
      " -3.3246\n",
      " -7.9050\n",
      " -6.9831\n",
      " -1.3906\n",
      " -0.0707\n",
      " -5.0964\n",
      " -5.9286\n",
      " -4.8269\n",
      " -3.3232\n",
      " -8.6734\n",
      " -0.0526\n",
      " -2.3419\n",
      " -1.8834\n",
      " -7.3734\n",
      " -1.5602\n",
      " -3.4248\n",
      " -1.1711\n",
      " -0.0221\n",
      " -2.5573\n",
      " -5.3520\n",
      " -5.2792\n",
      " -6.2121\n",
      " -1.4262\n",
      " -5.7440\n",
      " -5.2808\n",
      " -1.5317\n",
      " -4.4303\n",
      " -7.4350\n",
      " -0.9799\n",
      " -4.7446\n",
      " -8.2985\n",
      " -1.7155\n",
      " -1.4017\n",
      " -2.0969\n",
      " -2.9780\n",
      " -5.3707\n",
      " -6.6184\n",
      " -8.9761\n",
      " -0.0359\n",
      " -4.1743\n",
      " -0.0675\n",
      " -3.2403\n",
      " -3.8729\n",
      " -5.1544\n",
      "-10.0307\n",
      " -0.0574\n",
      " -4.0901\n",
      " -2.4703\n",
      " -7.8995\n",
      " -7.6559\n",
      " -3.2535\n",
      " -1.3818\n",
      " -3.2690\n",
      " -4.5187\n",
      " -2.7255\n",
      " -4.3168\n",
      "[torch.FloatTensor of size 64x1]\n",
      ", \n",
      " 0.7872  0.6015\n",
      " 1.7990  2.5153\n",
      " 1.8870  0.7816\n",
      " 1.8030  0.0756\n",
      "-2.2288 -1.1282\n",
      " 0.2780 -0.3405\n",
      "-3.1143 -1.4908\n",
      " 0.9741  1.4813\n",
      " 1.8396 -0.6212\n",
      "-2.7475 -1.0274\n",
      "-2.3944 -2.1850\n",
      "-1.1740  0.1202\n",
      " 0.2647 -0.2649\n",
      "-2.1724 -1.0263\n",
      " 2.2959  1.4852\n",
      "-2.1210 -0.9939\n",
      " 1.6876  1.1250\n",
      "-2.8554 -1.2062\n",
      " 0.1945 -0.4386\n",
      " 1.1556  1.9428\n",
      " 1.3751 -0.4391\n",
      "-2.6450 -0.7264\n",
      "-1.2634  0.6748\n",
      " 1.8706 -0.8014\n",
      " 0.9000  1.3001\n",
      " 0.1356 -0.3450\n",
      " 1.5707  0.0526\n",
      "-2.3087 -0.0742\n",
      "-2.3092  0.1127\n",
      "-2.2852 -1.1277\n",
      "-1.1783 -0.0848\n",
      " 1.9415  2.8517\n",
      "-2.2932 -0.0809\n",
      "-1.2080 -0.3428\n",
      " 1.8526  1.8250\n",
      " 2.2711  3.3362\n",
      " 0.8350  0.9562\n",
      " 2.1092  0.8223\n",
      "-2.7950 -0.9514\n",
      " 1.0584  1.6866\n",
      "-1.1931  0.3530\n",
      " 1.3876  0.3378\n",
      " 1.6314  0.7796\n",
      "-2.3130 -0.0857\n",
      " 1.6605  4.2611\n",
      "-2.9181 -1.2553\n",
      " 0.1705 -0.4807\n",
      " 2.0161  0.2513\n",
      " 0.2383 -0.2643\n",
      " 1.8040 -0.2663\n",
      " 1.6732  2.1634\n",
      " 1.9698  2.3438\n",
      "-2.8234 -3.2189\n",
      " 0.2165 -0.4370\n",
      " 1.9344  0.9466\n",
      " 1.5681 -0.3277\n",
      "-2.5201 -2.5139\n",
      "-2.6961 -1.0218\n",
      " 1.7992 -0.0958\n",
      "-1.1800  0.2610\n",
      " 1.5650  1.6500\n",
      " 2.0681  0.6062\n",
      " 1.5924  0.4330\n",
      " 2.0377  0.4323\n",
      "[torch.FloatTensor of size 64x2]\n",
      ", \n",
      "-0.9600\n",
      "-0.0400\n",
      "-1.8400\n",
      "-1.7600\n",
      "-1.5200\n",
      "-0.9600\n",
      "-1.8400\n",
      "-1.8000\n",
      "-1.9200\n",
      " 0.0000\n",
      " 0.3200\n",
      " 1.6000\n",
      "-0.9200\n",
      " 1.8400\n",
      "-1.9600\n",
      " 1.6000\n",
      "-1.8400\n",
      " 1.5200\n",
      " 0.0000\n",
      "-0.5600\n",
      "-1.5600\n",
      " 1.6400\n",
      " 1.6000\n",
      "-1.9200\n",
      "-1.8800\n",
      "-0.4400\n",
      "-1.9200\n",
      " 1.5600\n",
      " 1.1600\n",
      " 0.5600\n",
      " 1.8400\n",
      " 0.0000\n",
      " 1.6000\n",
      " 1.7200\n",
      "-1.6400\n",
      "-1.9600\n",
      "-1.8400\n",
      "-1.0400\n",
      " 0.8400\n",
      "-0.0400\n",
      " 1.6800\n",
      "-2.0000\n",
      "-1.8800\n",
      " 1.5600\n",
      " 0.4400\n",
      "-0.2800\n",
      "-0.5200\n",
      "-1.8800\n",
      "-0.8000\n",
      "-1.9600\n",
      "-2.0000\n",
      " 0.0000\n",
      "-1.0000\n",
      "-0.9200\n",
      "-1.9600\n",
      "-1.7600\n",
      "-0.0800\n",
      " 1.8800\n",
      "-1.6400\n",
      " 1.4800\n",
      "-2.0000\n",
      "-1.9600\n",
      "-1.9200\n",
      "-1.6800\n",
      "[torch.FloatTensor of size 64x1]\n",
      "]\n",
      "[\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     1\n",
      "    0     0     1     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     1     0     0\n",
      "    0     0     0     0     0     0     0     0     1     0\n",
      "    0     0     0     0     1     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     1\n",
      "    0     0     0     0     0     0     0     0     1     0\n",
      "    0     0     0     0     0     0     0     1     0     0\n",
      "    0     1     0     0     0     0     0     0     0     0\n",
      "    0     1     0     0     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     1     0     0\n",
      "    0     0     0     0     0     1     0     0     0     0\n",
      "    0     0     1     0     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     1     0     0     0     0     0     0\n",
      "    0     0     0     1     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     1     0     0     0     0     0     0     0     0\n",
      "    0     0     1     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     1     0     0\n",
      "    0     0     0     1     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     1\n",
      "    0     0     0     0     0     0     1     0     0     0\n",
      "    0     0     0     0     0     0     0     1     0     0\n",
      "    0     0     0     1     0     0     0     0     0     0\n",
      "    0     0     0     0     1     0     0     0     0     0\n",
      "    0     0     1     0     0     0     0     0     0     0\n",
      "    0     1     0     0     0     0     0     0     0     0\n",
      "    0     0     1     0     0     0     0     0     0     0\n",
      "    0     0     1     0     0     0     0     0     0     0\n",
      "    1     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     1     0\n",
      "    0     0     0     0     0     1     0     0     0     0\n",
      "    0     0     0     0     0     1     0     0     0     0\n",
      "[torch.FloatTensor of size 36x10]\n",
      ", \n",
      " 3.1068 -3.7483\n",
      " 0.7571  0.5280\n",
      "-2.3050 -0.0752\n",
      " 2.1566  0.9493\n",
      " 0.1528 -0.3534\n",
      " 2.1043  2.6904\n",
      " 1.4474  3.2165\n",
      " 0.2515 -0.2638\n",
      " 2.3782  1.6457\n",
      "-1.2972  0.7522\n",
      "-1.2325  0.6177\n",
      " 3.0774 -1.8288\n",
      " 1.9997 -0.1095\n",
      " 1.8198  0.3375\n",
      "-2.3170 -0.0807\n",
      "-3.0398 -1.3062\n",
      " 1.4263  0.7757\n",
      " 1.3971 -0.8467\n",
      "-2.9890 -3.3115\n",
      "-1.2107  0.4365\n",
      "-2.2892  0.1263\n",
      " 2.2216  1.3000\n",
      " 1.3707 -0.0897\n",
      " 1.2866  2.6202\n",
      "-2.6625 -2.8473\n",
      " 2.0036  0.0768\n",
      " 1.4825  1.1236\n",
      " 1.7614  1.4750\n",
      "-2.3012 -0.0743\n",
      "-1.1908 -0.2515\n",
      "-2.2955  0.2731\n",
      "-2.2975 -0.0858\n",
      "-2.9745 -1.1272\n",
      " 0.2950 -0.6399\n",
      " 1.8173 -0.4459\n",
      " 1.8480  0.5623\n",
      "[torch.FloatTensor of size 36x2]\n",
      ", \n",
      "-1.6800\n",
      "-2.0000\n",
      " 1.5600\n",
      "-1.0400\n",
      "-0.5200\n",
      " 0.0000\n",
      " 1.3600\n",
      "-0.9200\n",
      "-1.9600\n",
      " 1.9600\n",
      " 1.6000\n",
      "-1.8400\n",
      " 1.9535\n",
      "-1.7600\n",
      " 1.5600\n",
      "-0.5600\n",
      "-2.0000\n",
      "-1.6800\n",
      "-1.0000\n",
      " 1.8800\n",
      " 1.0000\n",
      " 0.6253\n",
      "-1.5600\n",
      "-0.5600\n",
      "-0.0800\n",
      "-1.9200\n",
      "-1.1200\n",
      "-0.1348\n",
      " 1.5600\n",
      " 1.8400\n",
      " 1.1600\n",
      " 1.6000\n",
      "-1.0223\n",
      " 0.4000\n",
      "-1.9200\n",
      "-1.7600\n",
      "[torch.FloatTensor of size 36x1]\n",
      ", \n",
      "-10.1932\n",
      " -0.6592\n",
      " -5.3321\n",
      " -5.1056\n",
      " -0.0304\n",
      " -6.2710\n",
      " -4.5744\n",
      " -0.0642\n",
      " -6.3291\n",
      " -1.6445\n",
      " -1.4867\n",
      " -9.2830\n",
      " -4.0180\n",
      " -3.4484\n",
      " -5.3877\n",
      " -9.9215\n",
      " -2.3281\n",
      " -1.9131\n",
      "-11.0578\n",
      " -1.4384\n",
      " -5.2602\n",
      " -5.4939\n",
      " -1.9391\n",
      " -3.1298\n",
      " -9.0077\n",
      " -4.0744\n",
      " -2.7228\n",
      " -3.7683\n",
      " -5.3150\n",
      " -1.4734\n",
      " -5.2428\n",
      " -5.2978\n",
      " -9.4110\n",
      " -0.0889\n",
      " -3.2636\n",
      " -3.6238\n",
      "[torch.FloatTensor of size 36x1]\n",
      ", \n",
      " 3.1068 -3.7483\n",
      " 0.7571  0.5280\n",
      "-2.3050 -0.0752\n",
      " 2.1566  0.9493\n",
      " 0.1528 -0.3534\n",
      " 2.1043  2.6904\n",
      " 1.4474  3.2165\n",
      " 0.2515 -0.2638\n",
      " 2.3782  1.6457\n",
      "-1.2972  0.7522\n",
      "-1.2325  0.6177\n",
      " 3.0774 -1.8288\n",
      " 1.9997 -0.1095\n",
      " 1.8198  0.3375\n",
      "-2.3170 -0.0807\n",
      "-3.0398 -1.3062\n",
      " 1.4263  0.7757\n",
      " 1.3971 -0.8467\n",
      "-2.9890 -3.3115\n",
      "-1.2107  0.4365\n",
      "-2.2892  0.1263\n",
      " 2.2216  1.3000\n",
      " 1.3707 -0.0897\n",
      " 1.2866  2.6202\n",
      "-2.6625 -2.8473\n",
      " 2.0036  0.0768\n",
      " 1.4825  1.1236\n",
      " 1.7614  1.4750\n",
      "-2.3012 -0.0743\n",
      "-1.1908 -0.2515\n",
      "-2.2955  0.2731\n",
      "-2.2975 -0.0858\n",
      "-2.9745 -1.1272\n",
      " 0.2950 -0.6399\n",
      " 1.8173 -0.4459\n",
      " 1.8480  0.5623\n",
      "[torch.FloatTensor of size 36x2]\n",
      ", \n",
      " 0.2800\n",
      "-0.8000\n",
      " 1.5200\n",
      "-1.5600\n",
      "-0.1200\n",
      "-0.8400\n",
      " 1.6800\n",
      "-1.7200\n",
      "-1.8000\n",
      " 2.0000\n",
      " 1.8800\n",
      "-1.7600\n",
      "-1.9200\n",
      "-1.7600\n",
      " 2.0000\n",
      "-1.6400\n",
      "-1.1200\n",
      "-1.9600\n",
      "-1.6800\n",
      " 1.8400\n",
      " 1.5600\n",
      "-1.5200\n",
      "-1.5200\n",
      " 1.3600\n",
      " 0.4400\n",
      "-1.8800\n",
      "-1.2000\n",
      "-0.9600\n",
      " 1.5600\n",
      " 1.8800\n",
      " 1.0000\n",
      " 1.5600\n",
      "-0.5600\n",
      "-0.6400\n",
      "-1.9600\n",
      "-1.9200\n",
      "[torch.FloatTensor of size 36x1]\n",
      "]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "     1     0     0     0     0     0     0     0     0     0\n",
       " [torch.FloatTensor of size 1x10], \n",
       "  2.5830  0.5264\n",
       " [torch.FloatTensor of size 1x2], \n",
       " -1.8400\n",
       " [torch.FloatTensor of size 1x1], \n",
       " -6.8538\n",
       " [torch.FloatTensor of size 1x1], \n",
       "  2.5830  0.5264\n",
       " [torch.FloatTensor of size 1x2], \n",
       " 1.00000e-02 *\n",
       "   8.0000\n",
       " [torch.FloatTensor of size 1x1]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
